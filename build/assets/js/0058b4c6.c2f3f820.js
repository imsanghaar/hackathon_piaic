"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/preface","label":"Preface: Welcome to the Era of Physical AI","docId":"preface","unlisted":false},{"type":"link","href":"/docs/introduction/","label":"Introduction","docId":"introduction/index","unlisted":false},{"type":"category","label":"Module 1: ROS 2 Fundamentals","items":[{"type":"link","href":"/docs/ros-2-fundamentals/ros-2-concepts","label":"ROS 2 Core Concepts and Fundamentals","docId":"ros-2-fundamentals/ros-2-concepts","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/ros-2-fundamentals/ros-2-overview"},{"type":"category","label":"Module 2: The Digital Twin","items":[{"type":"link","href":"/docs/digital-twin/simulation-with-gazebo","label":"Robot Simulation with Gazebo and Unity","docId":"digital-twin/simulation-with-gazebo","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/digital-twin/digital-twin-overview"},{"type":"category","label":"Module 3: NVIDIA Isaac Platform","items":[{"type":"link","href":"/docs/nvidia-isaac-platform/isaac-perception-training","label":"NVIDIA Isaac Platform for Perception and Training","docId":"nvidia-isaac-platform/isaac-perception-training","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/nvidia-isaac-platform/isaac-overview"},{"type":"category","label":"Module 4: VLA and Conversational Robotics","items":[{"type":"link","href":"/docs/vla-and-conversational-robotics/humanoid-robot-development","label":"Humanoid Robot Development and Conversational AI","docId":"vla-and-conversational-robotics/humanoid-robot-development","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/vla-and-conversational-robotics/vla-overview"},{"type":"link","href":"/docs/weekly-breakdown","label":"Weekly Breakdown","docId":"weekly-breakdown","unlisted":false},{"type":"link","href":"/docs/assessments","label":"Assessments","docId":"assessments","unlisted":false},{"type":"category","label":"Hardware Requirements","items":[{"type":"link","href":"/docs/hardware-requirements/","label":"Hardware Requirements for Physical AI & Humanoid Robotics","docId":"hardware-requirements/index","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/hardware-requirements/"}]},"docs":{"assessments":{"id":"assessments","title":"Assessments","description":"This section details the various assessment components designed to evaluate your understanding and practical application of the course material. These assessments are structured to reinforce learning and provide opportunities to demonstrate proficiency in key areas of Physical AI and humanoid robotics.","sidebar":"tutorialSidebar"},"digital-twin/digital-twin-overview":{"id":"digital-twin/digital-twin-overview","title":"The Digital Twin (Gazebo & Unity) Overview","description":"Explore the concept of a Digital Twin in robotics, focusing on its role in creating virtual replicas of physical systems for safe and efficient development. This module utilizes powerful simulation platforms like Gazebo and Unity to build, test, and refine humanoid robot behaviors in a controlled virtual environment.","sidebar":"tutorialSidebar"},"digital-twin/simulation-with-gazebo":{"id":"digital-twin/simulation-with-gazebo","title":"Robot Simulation with Gazebo and Unity","description":"Gazebo is an open-source 3D robot simulator known for its robust physics engine and tight ROS integration, ideal for validating robotic algorithms. Unity, a powerful game engine, offers advanced rendering and a growing robotics ecosystem for high-fidelity sensor simulation and AI training. This section explores the practical setup and utilization of both Gazebo and Unity for designing, testing, and refining robotic systems in virtual environments.","sidebar":"tutorialSidebar"},"hardware-requirements/index":{"id":"hardware-requirements/index","title":"Hardware Requirements for Physical AI & Humanoid Robotics","description":"This section details the essential hardware components and configurations required to build a capable Physical AI laboratory. It covers everything from high-performance workstations for simulation and AI training to specialized edge computing kits and various robot platforms, ensuring you have the right tools for hands-on development.","sidebar":"tutorialSidebar"},"introduction/index":{"id":"introduction/index","title":"Introduction: The Rise of Physical AI and Humanoid Robotics","description":"The Physical World Is About to Change","sidebar":"tutorialSidebar"},"nvidia-isaac-platform/isaac-overview":{"id":"nvidia-isaac-platform/isaac-overview","title":"The AI-Robot Brain (NVIDIA Isaac\u2122) Overview","description":"Dive into the NVIDIA Isaac platform, a comprehensive suite of tools, software, and hardware designed to accelerate AI-powered robotics. This module covers advanced perception, simulation, and autonomous capabilities, leveraging Isaac Sim and Isaac ROS to bring intelligent Physical AI applications to life.","sidebar":"tutorialSidebar"},"nvidia-isaac-platform/isaac-perception-training":{"id":"nvidia-isaac-platform/isaac-perception-training","title":"NVIDIA Isaac Platform for Perception and Training","description":"NVIDIA Isaac platform provides comprehensive tools for developing, simulating, and deploying AI-powered robots, with a strong focus on perception and training. This section delves into how Isaac ROS accelerates vision pipelines for tasks like object detection and pose estimation, and how Isaac Sim enables synthetic data generation in simulation environments. These capabilities are crucial for efficiently training robust AI models for humanoid robotics, bridging the gap between virtual experimentation and real-world deployment.","sidebar":"tutorialSidebar"},"preface":{"id":"preface","title":"Preface: Welcome to the Era of Physical AI","description":"The Journey from Digital to Physical Intelligence","sidebar":"tutorialSidebar"},"ros-2-fundamentals/ros-2-concepts":{"id":"ros-2-fundamentals/ros-2-concepts","title":"ROS 2 Core Concepts and Fundamentals","description":"ROS 2 (Robot Operating System 2) is a flexible framework providing tools, libraries, and conventions for building complex robotic applications. This section dives into its foundational elements: nodes, topics, services, actions, and parameters, which enable distributed, real-time, and secure communication among various components of a robotic system. Understanding these core concepts is essential for developing robust and scalable Physical AI solutions.","sidebar":"tutorialSidebar"},"ros-2-fundamentals/ros-2-overview":{"id":"ros-2-fundamentals/ros-2-overview","title":"The Robotic Nervous System (ROS 2) Overview","description":"This module introduces ROS 2 (Robot Operating System 2), the essential middleware for modern robotics. It covers core concepts like nodes, topics, services, and actions, and delves into how ROS 2 facilitates seamless communication and control for complex Physical AI and humanoid robot systems.","sidebar":"tutorialSidebar"},"vla-and-conversational-robotics/humanoid-robot-development":{"id":"vla-and-conversational-robotics/humanoid-robot-development","title":"Humanoid Robot Development and Conversational AI","description":"Humanoid Robot Development focuses on engineering robots that mimic human form and capabilities, tackling challenges like bipedal locomotion, advanced manipulation, and intuitive human-robot interaction. Conversational AI, through technologies like natural language processing and generation, enables these robots to understand and respond to human language, bridging the gap between physical action and intelligent dialogue. This section explores the convergence of these fields.","sidebar":"tutorialSidebar"},"vla-and-conversational-robotics/vla-overview":{"id":"vla-and-conversational-robotics/vla-overview","title":"Vision-Language-Action (VLA) Overview","description":"Discover Vision-Language-Action (VLA) models and the exciting convergence of large language models (LLMs) with robotics. This module explores how robots can perceive their environment (vision), understand human instructions (language), and translate these into meaningful physical actions, enabling intuitive human-robot interaction and sophisticated autonomous behaviors.","sidebar":"tutorialSidebar"},"weekly-breakdown":{"id":"weekly-breakdown","title":"Weekly Breakdown","description":"This section outlines the suggested weekly progression through the course material, designed to provide a structured learning path for mastering Physical AI and humanoid robotics. Each week builds upon the knowledge and skills acquired in previous weeks, culminating in a comprehensive understanding and practical application of the concepts.","sidebar":"tutorialSidebar"}}}}')}}]);