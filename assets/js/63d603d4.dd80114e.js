"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[2243],{2511:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"python-ai-development","title":"Chapter 13 - Python for AI Development","description":"Master Python for building production-ready AI applications in 2025","source":"@site/docs/13-python-ai-development.md","sourceDirName":".","slug":"/python-ai-development","permalink":"/hackathon_piaic/docs/python-ai-development","draft":false,"unlisted":false,"editUrl":"https://github.com/imsanghaar/piaic_hackathon_ai/edit/main/docs/13-python-ai-development.md","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"sidebar_position":13,"title":"Chapter 13 - Python for AI Development","description":"Master Python for building production-ready AI applications in 2025"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 12 - Google Antigravity","permalink":"/hackathon_piaic/docs/google-antigravity"},"next":{"title":"Chapter 14 - TypeScript for AI Development","permalink":"/hackathon_piaic/docs/typescript-ai-development"}}');var i=r(4848),a=r(8453);const o={sidebar_position:13,title:"Chapter 13 - Python for AI Development",description:"Master Python for building production-ready AI applications in 2025"},s="Chapter 13: Python for AI Development",l={},d=[{value:"13.1 Why Python for AI?",id:"131-why-python-for-ai",level:2},{value:"The Python Advantage",id:"the-python-advantage",level:3},{value:"Key Strengths",id:"key-strengths",level:3},{value:"13.2 Modern Python Development Practices",id:"132-modern-python-development-practices",level:2},{value:"Dependency Management: Beyond requirements.txt",id:"dependency-management-beyond-requirementstxt",level:3},{value:"Using Poetry",id:"using-poetry",level:4},{value:"Fast Virtual Environments",id:"fast-virtual-environments",level:3},{value:"Type Hinting for AI Code",id:"type-hinting-for-ai-code",level:3},{value:"Data Validation with Pydantic",id:"data-validation-with-pydantic",level:3},{value:"13.3 Essential AI/ML Libraries",id:"133-essential-aiml-libraries",level:2},{value:"Deep Learning Frameworks",id:"deep-learning-frameworks",level:3},{value:"PyTorch (Most Popular)",id:"pytorch-most-popular",level:4},{value:"TensorFlow/Keras",id:"tensorflowkeras",level:4},{value:"Hugging Face Transformers",id:"hugging-face-transformers",level:3},{value:"LangChain for AI Applications",id:"langchain-for-ai-applications",level:3},{value:"13.4 MLOps Best Practices",id:"134-mlops-best-practices",level:2},{value:"Version Control Everything",id:"version-control-everything",level:3},{value:"Experiment Tracking with MLflow",id:"experiment-tracking-with-mlflow",level:3},{value:"Data Validation with Great Expectations",id:"data-validation-with-great-expectations",level:3},{value:"Model Serving with FastAPI",id:"model-serving-with-fastapi",level:3},{value:"Continuous Monitoring",id:"continuous-monitoring",level:3},{value:"13.5 Testing AI/ML Code",id:"135-testing-aiml-code",level:2},{value:"Unit Testing with Pytest",id:"unit-testing-with-pytest",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"13.6 Ethical AI Development",id:"136-ethical-ai-development",level:2},{value:"Bias Detection and Mitigation",id:"bias-detection-and-mitigation",level:3},{value:"Privacy-Preserving ML",id:"privacy-preserving-ml",level:3},{value:"13.7 Performance Optimization",id:"137-performance-optimization",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Distributed Training",id:"distributed-training",level:3},{value:"Summary",id:"summary",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2},{value:"Theory Box: Core Concepts for Python AI Development",id:"theory-box-core-concepts-for-python-ai-development",level:2}];function c(e){const n={admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-13-python-for-ai-development",children:"Chapter 13: Python for AI Development"})}),"\n",(0,i.jsx)(n.p,{children:"Python continues to dominate the AI and machine learning landscape in 2025 due to its extensive ecosystem, simple syntax, and deep integration with major AI frameworks. This chapter covers modern best practices, tools, and patterns for building production-ready AI applications."}),"\n",(0,i.jsx)(n.h2,{id:"131-why-python-for-ai",children:"13.1 Why Python for AI?"}),"\n",(0,i.jsx)(n.h3,{id:"the-python-advantage",children:"The Python Advantage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Python's simplicity for complex AI tasks\r\nimport tensorflow as tf\r\n\r\n# Define a neural network in just a few lines\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-strengths",children:"Key Strengths"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rich Ecosystem"}),": TensorFlow, PyTorch, scikit-learn, Hugging Face"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Easy Syntax"}),": Focus on logic, not boilerplate"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Community"}),": Largest AI/ML developer community"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": Seamless connection with data tools"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexibility"}),": From prototyping to production"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"132-modern-python-development-practices",children:"13.2 Modern Python Development Practices"}),"\n",(0,i.jsx)(n.h3,{id:"dependency-management-beyond-requirementstxt",children:"Dependency Management: Beyond requirements.txt"}),"\n",(0,i.jsx)(n.admonition,{title:"2025 Best Practice",type:"tip",children:(0,i.jsxs)(n.p,{children:["Move from ",(0,i.jsx)(n.code,{children:"requirements.txt"})," to ",(0,i.jsx)(n.code,{children:"pyproject.toml"})," with modern tools like ",(0,i.jsx)(n.strong,{children:"Poetry"})," or ",(0,i.jsx)(n.strong,{children:"Rye"}),"."]})}),"\n",(0,i.jsx)(n.h4,{id:"using-poetry",children:"Using Poetry"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-toml",children:'# pyproject.toml\r\n[tool.poetry]\r\nname = "ai-project"\r\nversion = "0.1.0"\r\ndescription = "Production AI application"\r\n\r\n[tool.poetry.dependencies]\r\npython = "^3.11"\r\ntorch = "^2.0"\r\ntransformers = "^4.35"\r\nfastapi = "^0.104"\r\npydantic = "^2.0"\r\n\r\n[tool.poetry.group.dev.dependencies]\r\npytest = "^7.4"\r\nblack = "^23.0"\r\nmypy = "^1.7"\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install dependencies\r\npoetry install\r\n\r\n# Add new packages\r\npoetry add openai\r\n\r\n# Update packages\r\npoetry update\n"})}),"\n",(0,i.jsx)(n.h3,{id:"fast-virtual-environments",children:"Fast Virtual Environments"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Use 'uv' for lightning-fast virtual environments\r\npip install uv\r\n\r\n# Create environment (10x faster than venv)\r\nuv venv myenv\r\n\r\n# Install packages at incredible speed\r\nuv pip install torch transformers datasets\n"})}),"\n",(0,i.jsx)(n.h3,{id:"type-hinting-for-ai-code",children:"Type Hinting for AI Code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from typing import List, Dict, Optional, Tuple\r\nimport numpy as np\r\nimport torch\r\n\r\ndef train_model(\r\n    data: np.ndarray,\r\n    labels: np.ndarray,\r\n    epochs: int = 10,\r\n    learning_rate: float = 0.001\r\n) -> Tuple[torch.nn.Module, Dict[str, List[float]]]:\r\n    """\r\n    Train a neural network model.\r\n    \r\n    Args:\r\n        data: Training data array\r\n        labels: Training labels\r\n        epochs: Number of training epochs\r\n        learning_rate: Learning rate for optimizer\r\n        \r\n    Returns:\r\n        Trained model and training history\r\n    """\r\n    model = create_model()\r\n    history: Dict[str, List[float]] = {"loss": [], "accuracy": []}\r\n    \r\n    # Training loop\r\n    for epoch in range(epochs):\r\n        loss, acc = train_epoch(model, data, labels, learning_rate)\r\n        history["loss"].append(loss)\r\n        history["accuracy"].append(acc)\r\n    \r\n    return model, history\n'})}),"\n",(0,i.jsx)(n.h3,{id:"data-validation-with-pydantic",children:"Data Validation with Pydantic"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel, Field, validator\r\nfrom typing import List, Optional\r\n\r\nclass TrainingConfig(BaseModel):\r\n    """Configuration for model training with automatic validation."""\r\n    \r\n    model_name: str = Field(..., min_length=1)\r\n    batch_size: int = Field(32, gt=0, le=512)\r\n    learning_rate: float = Field(0.001, gt=0, lt=1)\r\n    epochs: int = Field(10, gt=0, le=1000)\r\n    optimizer: str = Field("adam", pattern="^(adam|sgd|adamw)$")\r\n    \r\n    @validator(\'learning_rate\')\r\n    def validate_lr(cls, v):\r\n        if v > 0.1:\r\n            raise ValueError("Learning rate too high, may cause instability")\r\n        return v\r\n\r\n# Usage\r\nconfig = TrainingConfig(\r\n    model_name="bert-base",\r\n    batch_size=64,\r\n    learning_rate=0.0001,\r\n    epochs=20\r\n)\r\n\r\n# Automatic validation prevents errors!\r\n# config = TrainingConfig(batch_size=-1)  # \u274c Raises ValidationError\n'})}),"\n",(0,i.jsx)(n.h2,{id:"133-essential-aiml-libraries",children:"13.3 Essential AI/ML Libraries"}),"\n",(0,i.jsx)(n.h3,{id:"deep-learning-frameworks",children:"Deep Learning Frameworks"}),"\n",(0,i.jsx)(n.h4,{id:"pytorch-most-popular",children:"PyTorch (Most Popular)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nclass TransformerModel(nn.Module):\r\n    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\r\n        super().__init__()\r\n        self.embedding = nn.Embedding(vocab_size, d_model)\r\n        self.transformer = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(d_model, nhead),\r\n            num_layers\r\n        )\r\n        self.fc = nn.Linear(d_model, vocab_size)\r\n    \r\n    def forward(self, x):\r\n        x = self.embedding(x)\r\n        x = self.transformer(x)\r\n        return self.fc(x)\r\n\r\n# Instantiate and train\r\nmodel = TransformerModel(vocab_size=50000)\r\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"tensorflowkeras",children:"TensorFlow/Keras"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Build model with functional API\r\ninputs = keras.Input(shape=(224, 224, 3))\r\nx = keras.layers.Conv2D(64, 3, activation='relu')(inputs)\r\nx = keras.layers.MaxPooling2D()(x)\r\nx = keras.layers.Conv2D(128, 3, activation='relu')(x)\r\nx = keras.layers.GlobalAveragePooling2D()(x)\r\noutputs = keras.layers.Dense(10, activation='softmax')(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n"})}),"\n",(0,i.jsx)(n.h3,{id:"hugging-face-transformers",children:"Hugging Face Transformers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from transformers import (\r\n    AutoTokenizer,\r\n    AutoModelForSequenceClassification,\r\n    TrainingArguments,\r\n    Trainer\r\n)\r\nfrom datasets import load_dataset\r\n\r\n# Load pre-trained model\r\nmodel_name = "bert-base-uncased"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\r\n    model_name,\r\n    num_labels=2\r\n)\r\n\r\n# Load and prepare dataset\r\ndataset = load_dataset("imdb")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples["text"], padding="max_length", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\r\n\r\n# Training configuration\r\ntraining_args = TrainingArguments(\r\n    output_dir="./results",\r\n    evaluation_strategy="epoch",\r\n    learning_rate=2e-5,\r\n    per_device_train_batch_size=16,\r\n    num_train_epochs=3,\r\n    weight_decay=0.01,\r\n)\r\n\r\n# Train with Trainer API\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=tokenized_datasets["train"],\r\n    eval_dataset=tokenized_datasets["test"],\r\n)\r\n\r\ntrainer.train()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"langchain-for-ai-applications",children:"LangChain for AI Applications"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from langchain.llms import OpenAI\r\nfrom langchain.prompts import PromptTemplate\r\nfrom langchain.chains import LLMChain, SequentialChain\r\nfrom langchain.memory import ConversationBufferMemory\r\n\r\n# Create prompt template\r\ntemplate = """\r\nYou are an expert Python developer. \r\nGiven this problem: {problem}\r\nProvide a solution with explanation.\r\n"""\r\n\r\nprompt = PromptTemplate(\r\n    input_variables=["problem"],\r\n    template=template\r\n)\r\n\r\n# Create chain\r\nllm = OpenAI(temperature=0.7)\r\nchain = LLMChain(llm=llm, prompt=prompt)\r\n\r\n# Use with memory for conversations\r\nmemory = ConversationBufferMemory()\r\nconversation_chain = LLMChain(\r\n    llm=llm,\r\n    prompt=prompt,\r\n    memory=memory\r\n)\r\n\r\n# Execute\r\nresult = chain.run(problem="Implement a binary search tree")\r\nprint(result)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"134-mlops-best-practices",children:"13.4 MLOps Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"version-control-everything",children:"Version Control Everything"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import dvc.api\r\n\r\n# Version control for data\r\n# Track large datasets without storing in git\r\n"""\r\n# Initialize DVC\r\ndvc init\r\n\r\n# Track data\r\ndvc add data/training_set.csv\r\ngit add data/training_set.csv.dvc\r\n\r\n# Push to remote storage\r\ndvc push\r\n"""\r\n\r\n# Access versioned data in code\r\nwith dvc.api.open(\'data/training_set.csv\', mode=\'r\') as f:\r\n    data = pd.read_csv(f)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"experiment-tracking-with-mlflow",children:"Experiment Tracking with MLflow"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import mlflow\r\nimport mlflow.pytorch\r\n\r\n# Start experiment\r\nmlflow.set_experiment("sentiment-analysis")\r\n\r\nwith mlflow.start_run():\r\n    # Log parameters\r\n    mlflow.log_param("learning_rate", 0.001)\r\n    mlflow.log_param("batch_size", 32)\r\n    mlflow.log_param("model", "bert-base")\r\n    \r\n    # Training code\r\n    model = train_model(data, epochs=10)\r\n    \r\n    # Log metrics\r\n    mlflow.log_metric("accuracy", 0.95)\r\n    mlflow.log_metric("f1_score", 0.93)\r\n    \r\n    # Log model\r\n    mlflow.pytorch.log_model(model, "model")\r\n    \r\n    # Log artifacts\r\n    mlflow.log_artifact("confusion_matrix.png")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"data-validation-with-great-expectations",children:"Data Validation with Great Expectations"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import great_expectations as gx\r\n\r\n# Create data context\r\ncontext = gx.get_context()\r\n\r\n# Define expectations\r\nvalidator = context.sources.pandas_default.read_csv(\r\n    "data/training_data.csv"\r\n)\r\n\r\n# Add expectations\r\nvalidator.expect_column_values_to_not_be_null("text")\r\nvalidator.expect_column_values_to_be_in_set("label", [0, 1])\r\nvalidator.expect_table_row_count_to_be_between(min_value=1000, max_value=1000000)\r\n\r\n# Validate\r\nresults = validator.validate()\r\n\r\nif not results.success:\r\n    raise ValueError("Data validation failed!")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"model-serving-with-fastapi",children:"Model Serving with FastAPI"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, HTTPException\r\nfrom pydantic import BaseModel\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n\r\napp = FastAPI(title="Sentiment Analysis API")\r\n\r\n# Load model at startup\r\nmodel_name = "sentiment-model"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\r\n\r\nclass PredictionRequest(BaseModel):\r\n    text: str\r\n\r\nclass PredictionResponse(BaseModel):\r\n    sentiment: str\r\n    confidence: float\r\n\r\n@app.post("/predict", response_model=PredictionResponse)\r\nasync def predict_sentiment(request: PredictionRequest):\r\n    """Predict sentiment of input text."""\r\n    try:\r\n        # Tokenize\r\n        inputs = tokenizer(\r\n            request.text,\r\n            return_tensors="pt",\r\n            padding=True,\r\n            truncation=True,\r\n            max_length=512\r\n        )\r\n        \r\n        # Predict\r\n        with torch.no_grad():\r\n            outputs = model(**inputs)\r\n            probs = torch.softmax(outputs.logits, dim=1)\r\n            confidence, predicted = torch.max(probs, 1)\r\n        \r\n        sentiment = "positive" if predicted.item() == 1 else "negative"\r\n        \r\n        return PredictionResponse(\r\n            sentiment=sentiment,\r\n            confidence=confidence.item()\r\n        )\r\n    \r\n    except Exception as e:\r\n        raise HTTPException(status_code=500, detail=str(e))\r\n\r\n@app.get("/health")\r\nasync def health_check():\r\n    return {"status": "healthy"}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"continuous-monitoring",children:"Continuous Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from evidently import ColumnMapping\r\nfrom evidently.report import Report\r\nfrom evidently.metrics import (\r\n    DatasetDriftMetric,\r\n    DatasetMissingValuesMetric,\r\n    ColumnDriftMetric\r\n)\r\n\r\ndef monitor_data_drift(reference_data, current_data):\r\n    """Monitor for data drift in production."""\r\n    \r\n    report = Report(metrics=[\r\n        DatasetDriftMetric(),\r\n        DatasetMissingValuesMetric(),\r\n        ColumnDriftMetric(column_name="text_length"),\r\n    ])\r\n    \r\n    report.run(\r\n        reference_data=reference_data,\r\n        current_data=current_data,\r\n        column_mapping=ColumnMapping()\r\n    )\r\n    \r\n    # Save report\r\n    report.save_html("monitoring/drift_report.html")\r\n    \r\n    # Check for drift\r\n    drift_result = report.as_dict()\r\n    if drift_result[\'metrics\'][0][\'result\'][\'dataset_drift\']:\r\n        send_alert("Data drift detected!")\r\n    \r\n    return drift_result\n'})}),"\n",(0,i.jsx)(n.h2,{id:"135-testing-aiml-code",children:"13.5 Testing AI/ML Code"}),"\n",(0,i.jsx)(n.h3,{id:"unit-testing-with-pytest",children:"Unit Testing with Pytest"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pytest\r\nimport torch\r\nimport numpy as np\r\nfrom your_model import preprocess_text, TextClassifier\r\n\r\ndef test_preprocess_text():\r\n    """Test text preprocessing function."""\r\n    input_text = "  Hello World!  "\r\n    result = preprocess_text(input_text)\r\n    \r\n    assert result == "hello world"\r\n    assert isinstance(result, str)\r\n    assert len(result) > 0\r\n\r\ndef test_model_output_shape():\r\n    """Test model output dimensions."""\r\n    model = TextClassifier(vocab_size=1000, num_classes=2)\r\n    batch_size = 8\r\n    seq_length = 128\r\n    \r\n    inputs = torch.randint(0, 1000, (batch_size, seq_length))\r\n    outputs = model(inputs)\r\n    \r\n    assert outputs.shape == (batch_size, 2)\r\n\r\n@pytest.mark.parametrize("text,expected", [\r\n    ("I love this!", "positive"),\r\n    ("This is terrible", "negative"),\r\n    ("It\'s okay", "neutral"),\r\n])\r\ndef test_sentiment_predictions(text, expected):\r\n    """Test sentiment predictions with multiple examples."""\r\n    model = load_trained_model()\r\n    prediction = model.predict(text)\r\n    assert prediction == expected\r\n\r\ndef test_model_reproducibility():\r\n    """Ensure model produces consistent results."""\r\n    torch.manual_seed(42)\r\n    model1 = TextClassifier()\r\n    output1 = model1(torch.randn(1, 128))\r\n    \r\n    torch.manual_seed(42)\r\n    model2 = TextClassifier()\r\n    output2 = model2(torch.randn(1, 128))\r\n    \r\n    assert torch.allclose(output1, output2)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import pytest\r\nfrom fastapi.testclient import TestClient\r\nfrom main import app\r\n\r\nclient = TestClient(app)\r\n\r\ndef test_prediction_endpoint():\r\n    """Test the prediction API endpoint."""\r\n    response = client.post(\r\n        "/predict",\r\n        json={"text": "This product is amazing!"}\r\n    )\r\n    \r\n    assert response.status_code == 200\r\n    data = response.json()\r\n    assert "sentiment" in data\r\n    assert "confidence" in data\r\n    assert data["sentiment"] in ["positive", "negative"]\r\n    assert 0 <= data["confidence"] <= 1\r\n\r\ndef test_health_endpoint():\r\n    """Test health check endpoint."""\r\n    response = client.get("/health")\r\n    assert response.status_code == 200\r\n    assert response.json() == {"status": "healthy"}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"136-ethical-ai-development",children:"13.6 Ethical AI Development"}),"\n",(0,i.jsx)(n.h3,{id:"bias-detection-and-mitigation",children:"Bias Detection and Mitigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from fairlearn.metrics import (\r\n    MetricFrame,\r\n    selection_rate,\r\n    demographic_parity_difference\r\n)\r\nfrom fairlearn.reductions import ExponentiatedGradient, DemographicParity\r\nimport pandas as pd\r\n\r\n# Analyze model fairness\r\ndef analyze_fairness(model, X_test, y_test, sensitive_features):\r\n    """Analyze model fairness across demographic groups."""\r\n    \r\n    predictions = model.predict(X_test)\r\n    \r\n    # Create metric frame\r\n    metric_frame = MetricFrame(\r\n        metrics={\r\n            \'accuracy\': accuracy_score,\r\n            \'selection_rate\': selection_rate\r\n        },\r\n        y_true=y_test,\r\n        y_pred=predictions,\r\n        sensitive_features=sensitive_features\r\n    )\r\n    \r\n    print("Metrics by group:")\r\n    print(metric_frame.by_group)\r\n    \r\n    # Calculate demographic parity\r\n    parity_diff = demographic_parity_difference(\r\n        y_true=y_test,\r\n        y_pred=predictions,\r\n        sensitive_features=sensitive_features\r\n    )\r\n    \r\n    if abs(parity_diff) > 0.1:\r\n        print(f"\u26a0\ufe0f Significant disparity detected: {parity_diff:.3f}")\r\n    \r\n    return metric_frame\r\n\r\n# Mitigate bias\r\ndef train_fair_model(X_train, y_train, sensitive_features):\r\n    """Train a model with fairness constraints."""\r\n    \r\n    base_model = LogisticRegression()\r\n    \r\n    # Apply fairness constraints\r\n    mitigator = ExponentiatedGradient(\r\n        base_model,\r\n        constraints=DemographicParity()\r\n    )\r\n    \r\n    mitigator.fit(X_train, y_train, sensitive_features=sensitive_features)\r\n    \r\n    return mitigator\n'})}),"\n",(0,i.jsx)(n.h3,{id:"privacy-preserving-ml",children:"Privacy-Preserving ML"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import opendp\r\nfrom opendp.measurements import make_laplace\r\nfrom opendp.transformations import make_clamp, make_bounded_sum\r\n\r\ndef private_mean(data, epsilon=1.0, bounds=(0, 100)):\r\n    """Calculate mean with differential privacy."""\r\n    \r\n    # Create privacy-preserving pipeline\r\n    clamp = make_clamp(bounds)\r\n    sum_transform = make_bounded_sum(bounds)\r\n    laplace_mech = make_laplace(sum_transform.output_space, epsilon)\r\n    \r\n    # Apply transformations\r\n    clamped_data = clamp(data)\r\n    noisy_sum = laplace_mech(sum_transform(clamped_data))\r\n    \r\n    private_mean = noisy_sum / len(data)\r\n    \r\n    return private_mean\r\n\r\n# Federated Learning setup\r\nfrom flwr import client, server\r\n\r\nclass AIClient(client.NumPyClient):\r\n    """Federated learning client for privacy-preserving training."""\r\n    \r\n    def get_parameters(self):\r\n        return get_model_parameters(self.model)\r\n    \r\n    def fit(self, parameters, config):\r\n        set_model_parameters(self.model, parameters)\r\n        self.model.fit(self.X_train, self.y_train, epochs=1)\r\n        return get_model_parameters(self.model), len(self.X_train), {}\r\n    \r\n    def evaluate(self, parameters, config):\r\n        set_model_parameters(self.model, parameters)\r\n        loss, accuracy = self.model.evaluate(self.X_test, self.y_test)\r\n        return loss, len(self.X_test), {"accuracy": accuracy}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"137-performance-optimization",children:"13.7 Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\r\n\r\n# Check GPU availability\r\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\nprint(f"Using device: {device}")\r\n\r\n# Move model and data to GPU\r\nmodel = model.to(device)\r\ninputs = inputs.to(device)\r\n\r\n# Mixed precision training for faster performance\r\nfrom torch.cuda.amp import autocast, GradScaler\r\n\r\nscaler = GradScaler()\r\n\r\nfor epoch in range(num_epochs):\r\n    for batch in dataloader:\r\n        inputs, labels = batch\r\n        inputs, labels = inputs.to(device), labels.to(device)\r\n        \r\n        optimizer.zero_grad()\r\n        \r\n        # Automatic mixed precision\r\n        with autocast():\r\n            outputs = model(inputs)\r\n            loss = criterion(outputs, labels)\r\n        \r\n        scaler.scale(loss).backward()\r\n        scaler.step(optimizer)\r\n        scaler.update()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"distributed-training",children:"Distributed Training"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch.distributed as dist\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\n\r\ndef setup(rank, world_size):\r\n    """Initialize distributed training."""\r\n    dist.init_process_group("nccl", rank=rank, world_size=world_size)\r\n\r\ndef train_distributed(rank, world_size):\r\n    """Train model across multiple GPUs."""\r\n    setup(rank, world_size)\r\n    \r\n    # Create model and move to GPU\r\n    model = YourModel().to(rank)\r\n    ddp_model = DDP(model, device_ids=[rank])\r\n    \r\n    # Training loop\r\n    for data, labels in dataloader:\r\n        data, labels = data.to(rank), labels.to(rank)\r\n        outputs = ddp_model(data)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n    \r\n    dist.destroy_process_group()\r\n\r\n# Launch with torchrun\r\n# torchrun --nproc_per_node=4 train.py\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Python remains the king of AI development in 2025:"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Modern Tooling"}),": Poetry, Pydantic, type hints",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Rich Ecosystem"}),": PyTorch, TensorFlow, Hugging Face",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"MLOps Integration"}),": MLflow, DVC, Great Expectations",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Production Ready"}),": FastAPI, containerization, monitoring",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Ethical AI"}),": Fairness, privacy, transparency tools",(0,i.jsx)(n.br,{}),"\n","\u2705 ",(0,i.jsx)(n.strong,{children:"Performance"}),": GPU acceleration, distributed training"]}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsxs)(n.p,{children:["In the next chapter, we'll explore ",(0,i.jsx)(n.strong,{children:"TypeScript for AI Development"})," - how JavaScript/TypeScript is becoming a powerful choice for AI applications, especially in web-based and full-stack contexts."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Takeaways:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use modern Python tooling (Poetry, Pydantic, type hints)"}),"\n",(0,i.jsx)(n.li,{children:"Implement MLOps from day one"}),"\n",(0,i.jsx)(n.li,{children:"Test AI/ML code comprehensively"}),"\n",(0,i.jsx)(n.li,{children:"Consider ethical implications (bias, privacy)"}),"\n",(0,i.jsx)(n.li,{children:"Optimize for performance (GPU, distributed training)"}),"\n",(0,i.jsx)(n.li,{children:"Build production-ready APIs with FastAPI"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"theory-box-core-concepts-for-python-ai-development",children:"Theory Box: Core Concepts for Python AI Development"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why this matters"})," \u2013 Python\u2019s simplicity and its rich ecosystem are the foundation of modern AI research and production."]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic Typing & Duck Typing"})," \u2013 Python determines an object\u2019s type at runtime, allowing rapid prototyping without boilerplate."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Tensor Operations"})," \u2013 Libraries like\u202fPyTorch expose tensors (",(0,i.jsx)(n.code,{children:"torch.Tensor"}),") that support automatic differentiation, enabling gradient\u2011based learning."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ecosystem Integration"})," \u2013 The same Python code can call C/C++ kernels, invoke GPU kernels, and be served through FastAPI, providing a seamless path from research to production."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>s});var t=r(6540);const i={},a=t.createContext(i);function o(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);